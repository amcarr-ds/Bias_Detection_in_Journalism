{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "72e49e21-4b1f-4603-a619-84b9e1998f03",
   "metadata": {},
   "source": [
    "# 509 Final Project"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81c57862-dbc2-4515-b62e-14adccbf6d2b",
   "metadata": {},
   "source": [
    "## Globally import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3c1e3ef-8619-4237-88c3-e4a06edaf6c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from collections import defaultdict, Counter\n",
    "import datetime as dt\n",
    "import emoji\n",
    "import itertools\n",
    "import json\n",
    "import logging\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import pymysql as mysql\n",
    "import random\n",
    "import re\n",
    "import regex as rex\n",
    "import requests\n",
    "import shutil\n",
    "from string import punctuation\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import zipfile\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import spacy\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfTransformer, CountVectorizer\n",
    "\n",
    "import textacy.preprocessing as tprep\n",
    "\n",
    "# Set pandas global options\n",
    "pd.options.display.max_rows = 17\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1823454-8427-4340-8a71-dd73fdf9932f",
   "metadata": {},
   "source": [
    "## Upload data from CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2e909d4-6cad-425c-8cee-2f0bfd303bfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Dir nav citation:\n",
    "https://softhints.com/python-change-directory-parent/'''\n",
    "curr_dir = os.path.abspath(os.curdir)\n",
    "print(curr_dir)\n",
    "os.chdir(\"..\")\n",
    "up1_dir = os.path.abspath(os.curdir)\n",
    "print(up1_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45f949b7-2dbb-4644-a792-35b2edf6c5fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# change `data_location` to the location of the folder on your machine.\n",
    "data_location = 'data'\n",
    "\n",
    "file_in_name01 = 'master.csv'\n",
    "\n",
    "file_in_path01 = os.path.join(up1_dir, data_location, file_in_name01)\n",
    "\n",
    "print(f'CSV file 1 in path: {file_in_path01}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bad30d7f-1009-4e26-b659-068edb48d048",
   "metadata": {},
   "source": [
    "### Review dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "001ea078-0f38-4692-b6bb-318957f1cb63",
   "metadata": {},
   "outputs": [],
   "source": [
    "slct_tbl_full_df01 = pd.read_csv(file_in_path01)\n",
    "print(f'Dataframe shape: {slct_tbl_full_df01.shape}')\n",
    "display(slct_tbl_full_df01.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "064029d1-d255-4cd4-b124-8d2fd98be38a",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis (EDA)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1638aba9-0192-4877-8cf7-927307a05917",
   "metadata": {},
   "source": [
    "### Count missing `article_text` feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "023f3505-3e5a-4675-8fda-dd795da00113",
   "metadata": {},
   "outputs": [],
   "source": [
    "count_nan = slct_tbl_full_df01.isnull().sum()\n",
    " \n",
    "# printing the number of values present\n",
    "# in the column\n",
    "print('Number of NaN values present: ' + str(count_nan))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf0406b0-bdd5-4527-a603-a787c2513864",
   "metadata": {},
   "source": [
    "### Count blank `article_text` feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af713abe-82b2-4132-9414-693d71ad3b9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(slct_tbl_full_df01[slct_tbl_full_df01['article_text']=='']))\n",
    "display(slct_tbl_full_df01[slct_tbl_full_df01['article_text']==''].head(20))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3227f0a-dead-400d-b7e5-9e71605d4a0d",
   "metadata": {},
   "source": [
    "### Remove missing `article_text` rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4b240a7-c114-46dd-abd7-9d0abfd38192",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Drop missing citation:\n",
    "https://pandas.pydata.org/pandas-docs/stable/reference\n",
    "/api/pandas.DataFrame.dropna.html#pandas.DataFrame.dropna'''\n",
    "slct_tbl_full_df02 = slct_tbl_full_df01.dropna(subset=['article_text'])\n",
    "print(f'Dataframe shape: {slct_tbl_full_df02.shape}')\n",
    "display(slct_tbl_full_df02.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8053ca17-e7a9-4f32-be0d-f57960a75d9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tqdm.pandas(ncols=50)  # can use tqdm_gui, optional kwargs, etc\n",
    "# Now you can use `progress_apply` instead of `apply`\n",
    "\n",
    "# Raw text character and word counts\n",
    "slct_tbl_full_df02['char_cnt'] = slct_tbl_full_df02['article_text'].progress_apply(len)\n",
    "slct_tbl_full_df02['word_cnt'] = slct_tbl_full_df02['article_text'].progress_apply(lambda x: len(x.split()))\n",
    "display(slct_tbl_full_df02.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5530b3c-6fa3-4bff-8530-ac8fcd278ced",
   "metadata": {},
   "source": [
    "### Descriptive statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02483fc7-e043-4ad0-85fa-556e6c00805d",
   "metadata": {},
   "outputs": [],
   "source": [
    "slct_tbl_full_df02[['source_name',\n",
    "                    'author',\n",
    "                    'publish_date',\n",
    "                    'article_text']].describe(include=\"O\").T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abd8cc29-4335-4d23-8bb0-a5aa007b6ec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "slct_tbl_full_df02.describe().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c2737f3-ef42-4a7a-a8d7-3bb0b92a689a",
   "metadata": {},
   "outputs": [],
   "source": [
    "slct_tbl_full_df02['source_name'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9750a38c-5e05-40bc-9e76-ac1d0628e7f7",
   "metadata": {},
   "source": [
    "### Examine inclusion of \"centrist\" sources indicated by `author` feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f2935e2-506f-44e4-b1df-c7c065042573",
   "metadata": {},
   "outputs": [],
   "source": [
    "slct_tbl_full_df02a = slct_tbl_full_df02[slct_tbl_full_df02['author'].isin(['msn', 'Associated Press', 'Reuters'])]\n",
    "\n",
    "display(slct_tbl_full_df02a[slct_tbl_full_df02a['author']=='msn'])\n",
    "\n",
    "display(slct_tbl_full_df02a.groupby(by=['source_name', 'author']).count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b12be800-e783-42a0-89e3-70050ac333ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "counter = Counter(slct_tbl_full_df02['author'])\n",
    "\n",
    "word_cutoff = 5\n",
    "con_feature_words = set()\n",
    "\n",
    "for word, count in counter.items():\n",
    "    if count > word_cutoff:\n",
    "        con_feature_words.add(word)\n",
    "        \n",
    "print(f\"With a word cutoff of {word_cutoff}, we have {len(con_feature_words)} words as features in the model.\")\n",
    "print(con_feature_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d870cad1-2c19-4702-a223-d1078aeebd49",
   "metadata": {},
   "outputs": [],
   "source": [
    "slct_tbl_full_df03 = slct_tbl_full_df02[~slct_tbl_full_df02['author'].isin(['msn', 'Associated Press', 'Reuters'])]\n",
    "slct_tbl_full_df03 = slct_tbl_full_df03.reset_index()\n",
    "slct_tbl_full_df03['political_lean'] = 'right'\n",
    "print(slct_tbl_full_df03.shape)\n",
    "display(slct_tbl_full_df03.head())\n",
    "slct_tbl_full_df03.loc[(slct_tbl_full_df03['source_name'] == 'The Washington Post') | (slct_tbl_full_df03['source_name'] == 'CNN'), 'political_lean'] = 'left'\n",
    "\n",
    "display(slct_tbl_full_df03.head())\n",
    "\n",
    "display(slct_tbl_full_df03['political_lean'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adc8cb5b-a6d0-401d-b6ec-860bf7a1c32c",
   "metadata": {},
   "outputs": [],
   "source": [
    "slct_tbl_full_df03['political_lean'].value_counts().plot(kind=\"bar\",\n",
    "                                                         legend=True,\n",
    "                                                         figsize=(5,6),\n",
    "                                                         title='Class distribution')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca6e8733-2b73-42b4-9185-97a7b5c44109",
   "metadata": {},
   "source": [
    "### Plot word counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee345790-72a0-4242-9567-cd87798b8a7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "slct_tbl_full_df03.groupby('source_name')['word_cnt'].plot(kind=\"hist\",\n",
    "                                                           density=True,\n",
    "                                                           alpha=0.5,\n",
    "                                                           legend=True,\n",
    "                                                           figsize=(15,9),\n",
    "                                                           title='Histogram of Word Count Frequencies',\n",
    "                                                           xlim=(0,6000))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f6f14e9-07b8-41e9-aea3-2d0753dd631d",
   "metadata": {},
   "source": [
    "## Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63ab3518-082e-4b50-b9ce-a51054572417",
   "metadata": {},
   "outputs": [],
   "source": [
    "def uniq_tok(df_col=None):\n",
    "    df_cols1 = pd.Series(df_col)\n",
    "\n",
    "    all_tokens_lst01 = []\n",
    "\n",
    "    [all_tokens_lst01.append(f) for f in df_cols1]\n",
    "    all_tokens_lst01 = list(itertools.chain.from_iterable(all_tokens_lst01))\n",
    "    all_tokens_set01 = set(all_tokens_lst01)\n",
    "    print(len(sorted(all_tokens_set01)))\n",
    "    print(sorted(all_tokens_set01))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caf76037-6948-4f58-b38f-ce07f417c7f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "slct_tbl_full_df04 = slct_tbl_full_df03.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc903dba-da13-45e1-8034-c435627741ee",
   "metadata": {},
   "source": [
    "### Case-loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "246411c2-ec95-4865-863c-c56152cd88a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "slct_tbl_full_df03['lower'] = slct_tbl_full_df03['article_text'].apply(str.lower)\n",
    "\n",
    "print(slct_tbl_full_df03.shape)\n",
    "display(slct_tbl_full_df03.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1b9b77a-262f-4a6c-8966-b019f93d2e9f",
   "metadata": {},
   "source": [
    "### Text normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df9a5086-7cc5-404f-a78d-c61dcac5d34d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(text):\n",
    "    text = tprep.normalize.hyphenated_words(text)\n",
    "    text = tprep.normalize.quotation_marks(text)\n",
    "    text = tprep.normalize.unicode(text)\n",
    "    text = tprep.remove.accents(text)\n",
    "    return text\n",
    "\n",
    "slct_tbl_full_df03['norm'] = slct_tbl_full_df03['lower'].apply(normalize)\n",
    "\n",
    "print(slct_tbl_full_df03.shape)\n",
    "display(slct_tbl_full_df03.head())\n",
    "\n",
    "for c in range(0,5):\n",
    "    try:\n",
    "        print(slct_tbl_full_df03['norm'][c], '\\n')\n",
    "    except:\n",
    "        print(f'Skip {c}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfb11e0e-f747-4b07-961b-c402c68d592a",
   "metadata": {},
   "source": [
    "### Remove special characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ff3c6a5-29f0-4765-a5b3-4978e81fd1fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Complex citation (add lambda):\n",
    "https://chat.openai.com/share/a135754c-c38c-47ea-8f83-54d41d5397ab'''\n",
    "rex_u_code = rex.compile(r'\\\\u20[\\w]{2}')\n",
    "slct_tbl_full_df03['replace'] = slct_tbl_full_df03['norm'].apply(lambda x: x.replace('&nbsp;', ' ').replace(r'\\n', ' ').replace('\\u2063', ' ').replace('\\u2066', ' ').replace('\\u2069', ' ').replace('\\u200b', ' ').replace('\\u200d', ' '))\n",
    "#slct_tbl_full_df03['replace_a'] = slct_tbl_full_df03['norm'].apply(lambda x: x.replace('&nbsp;', ' '))\n",
    "#slct_tbl_full_df03['replace'] = slct_tbl_full_df03['replace_a'].apply(lambda x: rex_u_code.sub(' ', x))\n",
    "#re.sub(pattern, replacement, string)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f1e4bc5-f53b-48a4-b878-5a4c40488181",
   "metadata": {},
   "source": [
    "### RegEx find"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3441d4df-ed11-43a4-a347-b1b6437c3305",
   "metadata": {},
   "outputs": [],
   "source": [
    "rex_url_c = rex.compile(r'http[s]?:[\\/]+[\\S]*\\s')\n",
    "#rex_url_c = rex.compile(r'\\n')\n",
    "'''re.sub lambda citation:\n",
    "https://chat.openai.com/share/402ec66e-2802-4cda-af8c-6f9f5b097d85'''\n",
    "# Add leading and trailing space to URLs\n",
    "def rex_url(text):\n",
    "    text = rex_url_c.sub(lambda match: ' ' + match.group(0) + ' ', text)\n",
    "    return text\n",
    "    \n",
    "slct_tbl_full_df03['rex_urls'] = slct_tbl_full_df03['replace'].apply(rex_url)\n",
    "\n",
    "print(slct_tbl_full_df03.shape)\n",
    "display(slct_tbl_full_df03.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1603dece-eae5-4187-a25c-adbc4d990c90",
   "metadata": {},
   "source": [
    "### Split emojis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18c395b7-1744-4ead-ab4c-70c8431cfe0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def emoji_split(text):\n",
    "    return(\"\".join([' ' + ch + ' ' if emoji.is_emoji(ch) else ch for ch in text]))\n",
    "\n",
    "#lambda x: x.replace(x, ' ' + x + ' ') if emoji.is_emoji(x) else x\n",
    "\n",
    "slct_tbl_full_df03['emoji_split'] = slct_tbl_full_df03['rex_urls'].apply(emoji_split)\n",
    "\n",
    "print(slct_tbl_full_df03.shape)\n",
    "display(slct_tbl_full_df03.head())\n",
    "\n",
    "for c in range(0,5):\n",
    "    try:\n",
    "        print(slct_tbl_full_df03['emoji_split'][c], '\\n')\n",
    "    except:\n",
    "        print(f'Skip {c}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cb223e0-a1ae-41a0-bf0e-72f38e526edb",
   "metadata": {},
   "source": [
    "#### Display globally unqiue emojis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f63367d2-9a80-4a86-8824-06a3671e1cab",
   "metadata": {},
   "source": [
    "uniq_tok(df_col=slct_tbl_full_df03['emoji_lst'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8cd9946-022f-4e74-9396-6b90fe015b90",
   "metadata": {},
   "source": [
    "### Lemmatization using spaCY"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "584b0f29-c069-476c-83f0-1215d68b8e33",
   "metadata": {},
   "source": [
    "nlp_trans01 = spacy.load('en_core_web_sm')\n",
    "\n",
    "def lemma(text):\n",
    "    trans_txt = nlp_trans01(text)\n",
    "    return [t.lemma_ for t in trans_txt]\n",
    "\n",
    "slct_tbl_full_df03['lemma'] = slct_tbl_full_df03['replace'].progress_apply(lemma)\n",
    "\n",
    "print(slct_tbl_full_df03.shape)\n",
    "display(slct_tbl_full_df03.head())\n",
    "\n",
    "for c in range(0,5):\n",
    "    try:\n",
    "        print(slct_tbl_full_df03['lemma'][c], '\\n')\n",
    "    except:\n",
    "        print(f'Skip {c}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8b5fde4-5525-4a81-88e0-6905f95bbc84",
   "metadata": {},
   "source": [
    "#### Display globally unqiue tokens on lemmas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88b5c2ca-0cc9-416c-a7b7-8f864970e7d1",
   "metadata": {},
   "source": [
    "uniq_tok(df_col=slct_tbl_full_df03['lemma'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9fbfaec-c4cc-4386-89ce-02870d1bbdef",
   "metadata": {},
   "source": [
    "### Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98cad2a1-a173-4f13-ac71-e3b408959b6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "slct_tbl_full_df03['split'] = slct_tbl_full_df03['emoji_split'].apply(str.split)\n",
    "\n",
    "print(slct_tbl_full_df03.shape)\n",
    "display(slct_tbl_full_df03.head())\n",
    "\n",
    "for c in range(0,100):\n",
    "    try:\n",
    "        print(slct_tbl_full_df03['split'][c], '\\n')\n",
    "    except:\n",
    "        print(f'Skip {c}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77efa83d-9c67-44b1-8422-c700a7ef52a2",
   "metadata": {},
   "source": [
    "#### Display globablly unqiue tokens on first split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60520bd0-1f52-44cf-bbb0-5480f93f6308",
   "metadata": {},
   "outputs": [],
   "source": [
    "uniq_tok(df_col=slct_tbl_full_df03['split'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6e472d7-dfdd-479f-9c04-f8647e19be52",
   "metadata": {},
   "source": [
    "### Remove stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58e808b5-0356-4c7b-a4c0-0c01d6a29d90",
   "metadata": {},
   "outputs": [],
   "source": [
    "sw = stopwords.words(\"english\")\n",
    "\n",
    "# Add additional stop words\n",
    "sw.extend(['',\n",
    "           '️',\n",
    "           'arent',\n",
    "           'cannot',\n",
    "           'cant',\n",
    "           'couldnt',\n",
    "           'couldve',\n",
    "           'didnt',\n",
    "           'doesnt',\n",
    "           'dont',\n",
    "           'hadnt',\n",
    "           'hasnt',\n",
    "           'havent',\n",
    "           'hes',\n",
    "           'im',\n",
    "           \"i'm\",\n",
    "           'isnt',\n",
    "           'it’s',\n",
    "           'ive',\n",
    "           '𝚘𝚏',\n",
    "           'mightnt',\n",
    "           'mustnt',\n",
    "           'neednt',\n",
    "           'shant',\n",
    "           'shes',\n",
    "           'shouldnt',\n",
    "           'shouldve',\n",
    "           'thatll',\n",
    "           'theyll',\n",
    "           'theyve',\n",
    "           'wasnt',\n",
    "           'werent',\n",
    "           'whats',\n",
    "           'weve',\n",
    "           'wont',\n",
    "           'wouldnt',\n",
    "           'wouldve',\n",
    "           'yall',\n",
    "           'youd',\n",
    "           'youll',\n",
    "           'youre',\n",
    "           'youve',\n",
    "           \"we'll\",\n",
    "           \"you’re\",\n",
    "           \"you’ve\",\n",
    "           \"you’ll\",\n",
    "           \"you’d\",\n",
    "           \"she’s\",\n",
    "           \"it’s\",\n",
    "           \"that’ll\",\n",
    "           \"don’t\",\n",
    "           \"should’ve\",\n",
    "           \"aren’t\",\n",
    "           \"couldn’t\",\n",
    "           \"didn’t\",\n",
    "           \"doesn’t\",\n",
    "           \"hadn’t\",\n",
    "           \"hasn’t\",\n",
    "           \"haven’t\",\n",
    "           \"isn’t\",\n",
    "           \"mightn’t\",\n",
    "           \"mustn’t\",\n",
    "           \"needn’t\",\n",
    "           \"shan’t\",\n",
    "           \"shouldn’t\",\n",
    "           \"wasn’t\",\n",
    "           \"weren’t\",\n",
    "           \"won’t\",\n",
    "           \"wouldn’t\",\n",
    "           \"i’m\",\n",
    "           \"we’ll\",\n",
    "           'said',\n",
    "           'told',\n",
    "           'according',\n",
    "           'fox',\n",
    "           'news',\n",
    "           'cnn',\n",
    "           'breitbart',\n",
    "           'reuters'])\n",
    "\n",
    "print(sw)\n",
    "\n",
    "def sw_remover(tokens):\n",
    "    return [t for t in tokens if t.lower() not in sw]\n",
    "\n",
    "slct_tbl_full_df03['no_sw'] = slct_tbl_full_df03['split'].apply(sw_remover)\n",
    "\n",
    "print(slct_tbl_full_df03.shape)\n",
    "display(slct_tbl_full_df03.head())\n",
    "\n",
    "for c in range(0,2):\n",
    "    print(slct_tbl_full_df03['no_sw'][c])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fbefe6a-cd44-4e84-9b55-4f9583a0efeb",
   "metadata": {},
   "source": [
    "#### Display no stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87415a4a-1aa5-4e9c-ad0c-51fdcc68bae9",
   "metadata": {},
   "outputs": [],
   "source": [
    "uniq_tok(df_col=slct_tbl_full_df03['no_sw'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40bf622a-0107-4c50-a905-bc218f1f46c4",
   "metadata": {},
   "source": [
    "### Rejoin semi-processed tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c883941-477e-489c-9f82-8e779ac89b08",
   "metadata": {},
   "outputs": [],
   "source": [
    "slct_tbl_full_df03['no_sw_join'] = slct_tbl_full_df03['no_sw'].apply(\" \".join)\n",
    "\n",
    "print(slct_tbl_full_df03.shape)\n",
    "display(slct_tbl_full_df03.head())\n",
    "\n",
    "for c in range(0,2):\n",
    "    print(slct_tbl_full_df03['no_sw_join'][c])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf03b56f-4e9f-4908-8cff-b171864b8b98",
   "metadata": {},
   "source": [
    "### Remove punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edf3c8e6-4f16-46e4-aecc-7fa83385918e",
   "metadata": {},
   "outputs": [],
   "source": [
    "punctuation = set(punctuation) # speeds up comparison\n",
    "print(punctuation)\n",
    "\n",
    "# Add special hyphen mark\n",
    "tw_punct = punctuation - {\"#\"}\n",
    "print(tw_punct)\n",
    "# Remove hash and at symbols for later capture of hashtag info\n",
    "tw_punct = tw_punct - {\"@\"}\n",
    "tw_punct = tw_punct - {\"-\"}\n",
    "tw_punct = tw_punct - {\"/\"}\n",
    "tw_punct.add(\"’\")\n",
    "tw_punct.add(\"‘\")\n",
    "tw_punct.add(\"”\")\n",
    "tw_punct.add(\"“\")\n",
    "tw_punct.add(\"…\")\n",
    "tw_punct.add(\"—\")\n",
    "tw_punct.add(\"...\")\n",
    "tw_punct.add(\"€\")\n",
    "tw_punct.add(\"±\")\n",
    "tw_punct.add(\"£\")\n",
    "tw_punct.add(\"¡\")\n",
    "tw_punct.add(\"§\")\n",
    "tw_punct.add(\"⦿\")\n",
    "\n",
    "print(tw_punct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb3bf84d-dfb1-49db-9fd1-9df5027bffaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punctuation(text, punct_set=punctuation): \n",
    "    return(\"\".join([ch for ch in text if ch not in punct_set]))\n",
    "\n",
    "slct_tbl_full_df03['no_sw_join_no_punc'] = slct_tbl_full_df03['no_sw_join'].apply(remove_punctuation, punct_set=tw_punct)\n",
    "\n",
    "print(slct_tbl_full_df03.shape)\n",
    "display(slct_tbl_full_df03.head())\n",
    "\n",
    "for c in range(0,10):\n",
    "    try:\n",
    "        print(slct_tbl_full_df03['no_sw_join_no_punc'][c], '\\n')\n",
    "    except:\n",
    "        print(f'\\nerror on {c}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d7644f8-60b3-48e9-aa05-2164563610bf",
   "metadata": {},
   "source": [
    "### Tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dca4d11-4ef6-40ae-997f-ba7860f7a1ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "slct_tbl_full_df03['no_sw_join_no_punc_tok'] = slct_tbl_full_df03['no_sw_join_no_punc'].apply(str.split)\n",
    "\n",
    "print(slct_tbl_full_df03.shape)\n",
    "display(slct_tbl_full_df03.head())\n",
    "\n",
    "for c in range(0,2):\n",
    "    print(slct_tbl_full_df03['no_sw_join_no_punc_tok'][c], '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c53bc528-38ed-4140-aeb2-2c70986ea7a3",
   "metadata": {},
   "source": [
    "#### Display globally unqiue tokens on final tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7963d626-18db-4d88-a58c-7c9c0ca88380",
   "metadata": {},
   "outputs": [],
   "source": [
    "uniq_tok(df_col=slct_tbl_full_df03['no_sw_join_no_punc_tok'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4e7fff5-d031-43a3-9688-eb37bff61ea8",
   "metadata": {},
   "source": [
    "### Pipeline consolidation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f967a027-ac93-4021-b248-09779d7004c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare(text, pipeline):\n",
    "    '''Run a pipeline of text processing transformers'''\n",
    "    tokens = str(text)\n",
    "    \n",
    "    # Pull key and val from trans dictionaries\n",
    "    for transformer in pipeline:\n",
    "        trans = list(transformer.keys())[0]\n",
    "        args = list(transformer.values())[0]\n",
    "        #print(trans)\n",
    "        #print(args)\n",
    "        if args == None:\n",
    "            #print(1)\n",
    "            tokens = trans(tokens)\n",
    "        else:\n",
    "            #print('check99', trans, args)\n",
    "            tokens = trans(tokens, args) \n",
    "\n",
    "    return(tokens)\n",
    "\n",
    "'''Set transformer pipeline 1:\n",
    "Caseloading, normalization (using textacy), special ch removal,\n",
    "split on whitespace, stop word removal, rejoin,\n",
    "remove custom punctuation, tokenize\n",
    "'''\n",
    "transformers01 = [{str.lower: None},\n",
    "                  {normalize: None},\n",
    "                  {(lambda x: x.replace('&nbsp;', ' ').replace(r'\\\\n', ' ').replace('\\u2063', ' ').replace('\\u2066', ' ').replace('\\u2069', ' ').replace('\\u200b', ' ').replace('\\u200d', ' ')): None},\n",
    "                  {rex_url: None},\n",
    "                  {emoji_split: None},\n",
    "                  {str.split: None},\n",
    "                  {sw_remover: None},\n",
    "                  {\" \".join: None},\n",
    "                  {remove_punctuation: tw_punct},\n",
    "                  {str.split: None},\n",
    "                  #{\" \".join: None},\n",
    "                 ]\n",
    "\n",
    "# Apply transformers to pandas dataframe, w/ new col containing tokens\n",
    "slct_tbl_full_df04['processed_text'] = slct_tbl_full_df04['article_text'].progress_apply(prepare,\n",
    "                                                                                 pipeline=transformers01)\n",
    "slct_tbl_full_df04['num_tokens'] = slct_tbl_full_df04['processed_text'].map(len)\n",
    "\n",
    "display(slct_tbl_full_df04.head())\n",
    "\n",
    "# Review unique tokens across entire dataset\n",
    "for c in range(0,5):\n",
    "    try:\n",
    "        print(slct_tbl_full_df04['processed_text'][c], '\\n')\n",
    "    except:\n",
    "        print(f'Skip {c}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51f6e5c8-9f2e-40a9-aecf-a02872a05c3e",
   "metadata": {},
   "source": [
    "### Calculate concentration ratio of each set of corpora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00e0d707-1f72-483c-8278-40ae9904ec7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(slct_tbl_full_df04['political_lean'].value_counts())\n",
    "\n",
    "slct_tbl_full_df04_left = slct_tbl_full_df04[slct_tbl_full_df04['political_lean'] == 'left']\n",
    "\n",
    "print(slct_tbl_full_df04_left.shape)\n",
    "#display(slct_tbl_full_df04_left.head())\n",
    "\n",
    "slct_tbl_full_df04_right = slct_tbl_full_df04[slct_tbl_full_df04['political_lean'] == 'right']\n",
    "\n",
    "print(slct_tbl_full_df04_right.shape)\n",
    "#display(slct_tbl_full_df04_right.head())\n",
    "\n",
    "slct_tbl_full_df04_left_s1 = list(itertools.chain.from_iterable(list(pd.Series(slct_tbl_full_df04_left['processed_text']))))\n",
    "print(slct_tbl_full_df04_left_s1[:10])\n",
    "slct_tbl_full_df04_right_s1 = list(itertools.chain.from_iterable(list(pd.Series(slct_tbl_full_df04_right['processed_text']))))\n",
    "print(slct_tbl_full_df04_right_s1[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1e01554-cc8e-4032-a8f1-9c2538f6ef00",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def concen_ratio(artist_lst=[],\n",
    "                 lsts=[]):\n",
    "    lyr_corp_lst = []\n",
    "    for l in lsts:\n",
    "        print(type(l))\n",
    "        lyr_corp_lst.append(' '.join(l))\n",
    "    print(len(lyr_corp_lst))\n",
    "    #print(lyr_corp_lst)\n",
    "\n",
    "    cv = CountVectorizer(input='content',\n",
    "                         encoding='utf-8',\n",
    "                         stop_words=None,\n",
    "                         token_pattern=r'\\S+'\n",
    "                        )\n",
    "\n",
    "    lyr_tokens_fit = cv.fit(lyr_corp_lst)\n",
    "\n",
    "    print(pd.Series(cv.get_feature_names_out()).sample(15))\n",
    "\n",
    "    lyr_tokens_sm = cv.transform(lyr_corp_lst)\n",
    "    display(lyr_tokens_sm)\n",
    "\n",
    "    df = pd.DataFrame(lyr_tokens_sm.toarray(),\n",
    "                      columns=cv.get_feature_names_out())\n",
    "    #display(df)\n",
    "\n",
    "    df02 = df.copy()\n",
    "    df02['r_sum'] = df02.sum(axis=1)\n",
    "    #display(df02)\n",
    "\n",
    "    '''Filter by frequency for all columns citation:\n",
    "    OpenAI. (2021). ChatGPT [Computer software]. https://openai.com/;\n",
    "    https://pandas.pydata.org/pandas-docs/stable/reference/api/\n",
    "    pandas.DataFrame.ge.html'''\n",
    "    condition = df.ge(5).all()\n",
    "    #print(condition)\n",
    "\n",
    "    # Get the list of columns that satisfy the condition\n",
    "    columns = condition[condition].index.tolist()\n",
    "    #print(columns)\n",
    "    columns.append('r_sum')\n",
    "    #print(columns)\n",
    "\n",
    "    #display(df02[columns])\n",
    "\n",
    "    df03 = df02[columns].copy()\n",
    "    display(df03)\n",
    "\n",
    "    '''Filter by frequency for all columns & add summary row citation:\n",
    "    OpenAI. (2021). ChatGPT [Computer software]. https://openai.com/'''\n",
    "    df04 = df03.apply(lambda x: x / df03.iloc[:,-1], axis=0)\n",
    "    #display(df04)\n",
    "\n",
    "    # Create new rows by dividing one artist row by the second artists row\n",
    "    new_row01 = df04.iloc[0] / df04.iloc[1]\n",
    "    new_row02 = df04.iloc[1] / df04.iloc[0]\n",
    "\n",
    "    # Append the new row to the DataFrame\n",
    "    df04 = df04.append(new_row01, ignore_index=True)\n",
    "    df04 = df04.append(new_row02, ignore_index=True)\n",
    "    display(df04)\n",
    "\n",
    "    # Transpose dataframe\n",
    "    df05 = df04.T\n",
    "    df05 = df05.reset_index()\n",
    "    df05.columns = ['token',\n",
    "                    'c1_concen',\n",
    "                    'c2_concen',\n",
    "                    'c1c2_concen_ratio',\n",
    "                    'c2c1_concen_ratio']\n",
    "    #print(df05)\n",
    "    \n",
    "    '''Sort values citation:\n",
    "    https://pandas.pydata.org/pandas-docs/stable/reference/api\n",
    "    /pandas.DataFrame.sort_values.html'''\n",
    "    print(artist_lst[0])\n",
    "    display(df05[['token',\n",
    "                  'c1c2_concen_ratio']].sort_values(by='c1c2_concen_ratio',\n",
    "                                                    ascending=False).head(10))\n",
    "    print(artist_lst[1])\n",
    "    display(df05[['token',\n",
    "                  'c2c1_concen_ratio']].sort_values(by='c2c1_concen_ratio',\n",
    "                                                    ascending=False).head(10))\n",
    "\n",
    "concen_ratio(artist_lst=['Left-Right Concentration Ratio',\n",
    "                         'Right-Left Concentration Ratio'],\n",
    "             lsts=[slct_tbl_full_df04_left_s1,\n",
    "                   slct_tbl_full_df04_right_s1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebc22a12-a22a-472d-86d6-594aa1276119",
   "metadata": {},
   "source": [
    "#### Display globally unqiue tokens on final tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cf661de-b023-41c5-8dbb-34e65d2c5783",
   "metadata": {},
   "outputs": [],
   "source": [
    "uniq_tok(df_col=slct_tbl_full_df04[\"processed_text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a5ecd7a-9351-4220-9f96-df300639f65e",
   "metadata": {},
   "source": [
    "### "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15e1d683-9c38-4ffe-9dba-0e8d5c6813ae",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
