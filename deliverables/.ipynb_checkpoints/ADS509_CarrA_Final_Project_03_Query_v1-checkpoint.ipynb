{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "72e49e21-4b1f-4603-a619-84b9e1998f03",
   "metadata": {},
   "source": [
    "# 509 Final Project"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57bb0078-81b9-44bf-b320-7f4c7432892f",
   "metadata": {},
   "source": [
    "This notebook reads in a CSV file that contain an attribute with raw HTML, then parses the article text using Beautiful Soup. A copy of the full DF is written to a CSV file for sharing with all colloborators in GitHub."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81c57862-dbc2-4515-b62e-14adccbf6d2b",
   "metadata": {},
   "source": [
    "## Globally import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f3c1e3ef-8619-4237-88c3-e4a06edaf6c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pymysql as mysql\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import shutil\n",
    "import re\n",
    "import logging\n",
    "import time\n",
    "import datetime as dt\n",
    "import zipfile\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import datetime\n",
    "import re\n",
    "import regex as rex\n",
    "from collections import defaultdict, Counter\n",
    "import random\n",
    "import json\n",
    "#import mysql.connector\n",
    "\n",
    "# Set pandas global options\n",
    "pd.options.display.max_rows = 17"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1823454-8427-4340-8a71-dd73fdf9932f",
   "metadata": {},
   "source": [
    "## Upload data from CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f2e909d4-6cad-425c-8cee-2f0bfd303bfa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\acarr\\Documents\\GitHub\\ADS509_Final_project\\deliverables\n",
      "C:\\Users\\acarr\\Documents\\GitHub\\ADS509_Final_project\n"
     ]
    }
   ],
   "source": [
    "'''Dir nav citation:\n",
    "https://softhints.com/python-change-directory-parent/'''\n",
    "curr_dir = os.path.abspath(os.curdir)\n",
    "print(curr_dir)\n",
    "os.chdir(\"..\")\n",
    "up1_dir = os.path.abspath(os.curdir)\n",
    "print(up1_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "45f949b7-2dbb-4644-a792-35b2edf6c5fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV file in path: C:\\Users\\acarr\\Documents\\GitHub\\ADS509_Final_project\\data_restricted\\data_raw_amc.csv\n",
      "CSV file out path: C:\\Users\\acarr\\Documents\\GitHub\\ADS509_Final_project\\data\\data_parsed_amc.csv\n"
     ]
    }
   ],
   "source": [
    "# change `data_location` to the location of the folder on your machine.\n",
    "data_r_location = 'data_restricted'\n",
    "data_location = 'data'\n",
    "\n",
    "file_in_name = 'data_raw_amc.csv'\n",
    "file_out_name = 'data_parsed_amc.csv'\n",
    "\n",
    "file_in_path01 = os.path.join(up1_dir, data_r_location, file_in_name)\n",
    "file_out_path = os.path.join(up1_dir, data_location, file_out_name)\n",
    "\n",
    "print(f'CSV file in path: {file_in_path01}')\n",
    "print(f'CSV file out path: {file_out_path}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "001ea078-0f38-4692-b6bb-318957f1cb63",
   "metadata": {},
   "outputs": [],
   "source": [
    "slct_tbl_full_df01 = pd.read_csv(file_in_path01)\n",
    "display(slct_tbl_full_df01.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9835939-644b-4cb1-b1e5-6d815ac7849a",
   "metadata": {},
   "source": [
    "## Extract article data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92670fe1-e1a0-4ada-9d5d-2d2d225d3d2d",
   "metadata": {},
   "source": [
    "### Check for missing articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "023f3505-3e5a-4675-8fda-dd795da00113",
   "metadata": {},
   "outputs": [],
   "source": [
    "count_nan = slct_tbl_full_df01['article_text'].isnull().sum()\n",
    " \n",
    "# printing the number of values present\n",
    "# in the column\n",
    "print('Number of NaN values present: ' + str(count_nan))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce62bec6-1749-4710-8010-77e9576d777b",
   "metadata": {},
   "source": [
    "### Parse the article text from the column with raw HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3af3f4d2-e668-430c-a53b-6d9551ebac24",
   "metadata": {},
   "outputs": [],
   "source": [
    "slct_tbl_full_df02 = slct_tbl_full_df01.copy()\n",
    "#slct_tbl_full_df02 = slct_tbl_full_df01.sort_values(by=['source_name'])\n",
    "#slct_tbl_full_df02 = slct_tbl_full_df02.reset_index()\n",
    "\n",
    "print(f'DF instances: {len(slct_tbl_full_df02)}')\n",
    "\n",
    "slct_tbl_full_df02['article_parsed'] = ''\n",
    "\n",
    "total_urls = len(slct_tbl_full_df02)\n",
    "\n",
    "# Start timer\n",
    "start_time = dt.datetime.today()\n",
    "\n",
    "# Use multiple parsing methods to extract the article from raw HTML\n",
    "for i, row in enumerate(slct_tbl_full_df02.itertuples(), 1):\n",
    "    #print(f'Enumeration #: {i}')\n",
    "    #print(row[7])\n",
    "    soup = BeautifulSoup(row[7], 'html.parser')\n",
    "\n",
    "    # Check for available JSON object\n",
    "    try:\n",
    "        script_tag = soup.find('script', {'type': 'application/ld+json'})\n",
    "        if script_tag == None:\n",
    "            json_err01 = f'''JSON Object = None: Index: {i-1}; source: {row[2]};\n",
    "            URL: {row[5]}'''\n",
    "        else:\n",
    "            article_json = json.loads(script_tag.string)\n",
    "            article_content = article_json['articleBody']\n",
    "            slct_tbl_full_df02.at[row.Index, 'article_parsed'] = article_content\n",
    "\n",
    "    # If no JSON object available, use Beautiful Soup to look for available \n",
    "    # HTML tags\n",
    "    except TypeError:\n",
    "        print(f'Type Error')\n",
    "    \n",
    "    except KeyError:\n",
    "        json_err02 = f'''Missing JSON key: Index: {i-1}; source: {row[2]};\n",
    "        URL: {row[5]}'''\n",
    "        article_body = soup.find('div', class_='article__content-container')\n",
    "\n",
    "        if article_body is None: #forfoxandbreitbert(sometimes)\n",
    "            #print('Class != article__content-container')\n",
    "            article_body = soup.find('p', class_=\"speakable\")\n",
    "            if article_body is None: #breitbert(most)\n",
    "                #print('Class != speakable')\n",
    "                article_body = soup.find('div', class_='entry-content')\n",
    "                if article_body is None: #WashPost\n",
    "                    #print('Class != entry-content')\n",
    "                    article_body = soup.find('div',class_='article-body')\n",
    "\n",
    "        if article_body is not None:\n",
    "            article_text = article_body.get_text()\n",
    "            slct_tbl_full_df02.at[row.Index, 'article_parsed'] = article_text\n",
    "            #print('Rejoice, parse was successful!')\n",
    "        else:\n",
    "            print('\\nParse not successful...')\n",
    "            try:\n",
    "                print(json_err02)\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "    print('.', end='')\n",
    "        \n",
    "# End timer script\n",
    "end_time = dt.datetime.today()\n",
    "time_elapse = end_time - start_time\n",
    "print(f'Start Time = {start_time}')\n",
    "print(f'End Time = {end_time}')\n",
    "print(f'Elapsed Time = {time_elapse}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aef35ec-2bb4-4c8d-9b4d-89392eea7aeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Review sample\n",
    "display(slct_tbl_full_df02.iloc[6119])\n",
    "#display(slct_tbl_full_df02.iloc[6119]['article_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c039be5-190c-40bb-b1bd-aebe5b909c2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(slct_tbl_full_df02[slct_tbl_full_df02['article_parsed']==''].head(11))\n",
    "print(len(slct_tbl_full_df02[slct_tbl_full_df02['article_parsed']=='']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a1ce2b1-a0fd-4db3-9fee-2d1799bf935d",
   "metadata": {},
   "outputs": [],
   "source": [
    "count_p_nan = slct_tbl_full_df02['article_parsed'].isnull().sum()\n",
    " \n",
    "# printing the number of values present\n",
    "# in the column\n",
    "print('Number of NaN values present: ' + str(count_p_nan))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d0b4623-d362-4df2-b500-9b56460ac7ae",
   "metadata": {},
   "source": [
    "### Write datafrme to CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ae7ef75-3c3d-41b2-bf7a-eff1e07c77fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "slct_tbl_full_df03 = slct_tbl_full_df02[['source_name',\n",
    "                                         'author',\n",
    "                                         'title',\n",
    "                                         'url',\n",
    "                                         'publish_date',\n",
    "                                         'article_parsed']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92b8b8a9-7c80-4a6e-8672-e67db0d559a5",
   "metadata": {},
   "source": [
    "slct_tbl_full_df03.to_csv(file_out_path, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
