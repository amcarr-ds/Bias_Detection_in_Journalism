{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d1e3473e",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9225b1e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "import logging\n",
    "from newsapi import NewsApiClient\n",
    "import os\n",
    "import pandas as pd\n",
    "import random\n",
    "import re\n",
    "import requests\n",
    "import string\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5ae5da6",
   "metadata": {},
   "source": [
    "## Source URLs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65b4ed93",
   "metadata": {},
   "outputs": [],
   "source": [
    "api_key = os.environ['NewsAPIKey']\n",
    "newsapi = NewsApiClient(api_key=api_key)\n",
    "\n",
    "sources = newsapi.get_sources(language='en', country='us')\n",
    "print(' - '.join([source['id'] for source in sources['sources']]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91d890a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def news_api_urls(q=None,\n",
    "                  s=None,\n",
    "                  d_from='2023-05-01',\n",
    "                  d_to='2023-05-31',\n",
    "                  api_lst=[]):\n",
    "    all_articles = newsapi.get_everything(q=q,\n",
    "                                          sources=s,\n",
    "                                          from_param=d_from,\n",
    "                                          to=d_to,\n",
    "                                          language='en',\n",
    "                                          sort_by='relevancy',\n",
    "                                          page=1)\n",
    "\n",
    "    for article in all_articles['articles']:\n",
    "        print('Title:', article['title'])\n",
    "\n",
    "    source_data01 = [(a['source']['name'],\n",
    "                      a['author'],\n",
    "                      a['title'],\n",
    "                      a['url'],\n",
    "                      a['publishedAt'],\n",
    "                      a['content'])\n",
    "                     for a in all_articles['articles']]\n",
    "\n",
    "    api_lst.extend(source_data01)\n",
    "    print(len(api_lst))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "320026d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Already executed:\n",
    "# run 'A': 'the-washington-post', '2023-05-30'\n",
    "# run 'B': 'the-washington-post', '2023-05-20/29'\n",
    "# run 'C': 'the-washington-post', '2023-05-21/22/23/24/25/26/27/28'\n",
    "# run 'D': 'the-washington-post', '2023-05-10/11/12/13/14/15/16/17/18/19'\n",
    "# run 'E': 'the-washington-post', '2023-05-05/06/07/08/09/31', '2023-06-01/02/03'\n",
    "# run 'F': 'fox-news', '2023-05-24/25/26/27/28'\n",
    "# run 'G': 'breitbart-news', '2023-05-24/25/26/27/28'\n",
    "# run 'H': 'cnn', '2023-05-24/25/26/27/28'\n",
    "\n",
    "# Last to execute:\n",
    "#run = 'H'\n",
    "#source_lst = ['cnn']\n",
    "#date_lst = ['2023-05-24', '2023-05-25', '2023-05-26', '2023-05-27', '2023-05-28']\n",
    "\n",
    "q_word_lst = ['justice OR surveillance', 'healthcare OR \"health care\"',\n",
    "              '(political AND (bias OR party)) OR republican OR democrat OR election',\n",
    "              'security AND (social OR national)']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89da81b4",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "api_record_lst01 = []\n",
    "for s in source_lst:\n",
    "    print(f'Source: {s}')\n",
    "    for d in date_lst:\n",
    "        print(f'Date: {d}')\n",
    "        for q in q_word_lst:\n",
    "            print(f'Query word: {q}')\n",
    "            time.sleep(5 * random.random())\n",
    "            news_api_urls(q=q,\n",
    "                          s=s,\n",
    "                          d_from=d,\n",
    "                          d_to=d,\n",
    "                          api_lst=api_record_lst01)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c3e861c",
   "metadata": {},
   "source": [
    "## Scrape articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb31d6a8",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "api_df = pd.DataFrame(api_record_lst01, columns=['source_name', 'author', 'title', 'url', 'publishedAt', 'content'])\n",
    "api_df['article_text'] = ''\n",
    "\n",
    "total_urls = len(api_df)\n",
    "for i, row in enumerate(api_df.itertuples(), 1):\n",
    "    print(f'Retrieving url {i} of {total_urls}...', end='')\n",
    "    response = requests.get(row.url)\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        print('; now Scraping...', end='')\n",
    "\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "        # The Washington Post\n",
    "#        try:\n",
    "#            script_tag = soup.find('script', {'type': 'application/ld+json'})\n",
    "#            article_json = json.loads(script_tag.string)\n",
    "#            article_content = article_json['hasPart']['value']\n",
    "#            api_df.at[row.Index, 'article_text'] = article_content\n",
    "      \n",
    "        # Fox News\n",
    "#        try:\n",
    "#            script_tag = soup.find('script', {'type': 'application/ld+json'})\n",
    "#            article_json = json.loads(script_tag.string)\n",
    "#            article_content = article_json['articleBody']\n",
    "#            api_df.at[row.Index, 'article_text'] = article_content\n",
    "\n",
    "        # Breitbart News\n",
    "#        try:\n",
    "#            title_tag = soup.find('h1')\n",
    "#            if title_tag is not None:\n",
    "#                title = title_tag.text\n",
    "#            content_tags = soup.find_all(['p', 'blockquote'])\n",
    "#            content = \" \".join([tag.text for tag in content_tags if tag.text.strip() != ''])\n",
    "#            api_df.at[row.Index, 'article_text'] = content\n",
    "\n",
    "        # CNN News\n",
    "        try:\n",
    "            script_tag = soup.find('script', {'type': 'application/ld+json'})\n",
    "            article_json = json.loads(script_tag.string)\n",
    "            article_content = article_json['articleBody']\n",
    "            api_df.at[row.Index, 'article_text'] = article_content\n",
    "\n",
    "        except KeyError:\n",
    "            print('; missing key in article JSON!', end='')\n",
    "\n",
    "        time.sleep(5 * random.random())\n",
    "        print('; done!')\n",
    "    else:\n",
    "        print(f'  response is {response.status_code}!')\n",
    "        \n",
    "api_df.to_csv(f'509_final_proj-{run}.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36a5799f",
   "metadata": {},
   "source": [
    "## Combine output files (*only at end of iterative process*)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21a431b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine all the output files\n",
    "master_df = pd.DataFrame()\n",
    "for letter in string.ascii_lowercase:\n",
    "    file_name = f'509_final_proj-{letter.upper()}.csv'\n",
    "    \n",
    "    if os.path.isfile(file_name):\n",
    "        df = pd.read_csv(file_name)\n",
    "\n",
    "        master_df = pd.concat([master_df, df], ignore_index=True)\n",
    "    else:\n",
    "        break\n",
    "print(master_df.info())\n",
    "\n",
    "# Get rid of what appear to be very cluttered (misread) article text rows\n",
    "pattern = re.compile('\\n{3,}')\n",
    "rows_list = []\n",
    "for index, row in master_df.iterrows():\n",
    "    try:\n",
    "        processed_row = row\n",
    "        if not pattern.search(str(processed_row[6])):\n",
    "            rows_list.append(processed_row)\n",
    "    except Exception as e:\n",
    "        print(f'Error processing row {index}: {e}')\n",
    "print(f'Rows processed = {len(rows_list)}')\n",
    "new_df = pd.concat(rows_list, axis=1).transpose()\n",
    "new_df.columns = master_df.columns\n",
    "              \n",
    "# Save the new file\n",
    "new_df.to_csv('509_final_proj.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed42fff8",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
