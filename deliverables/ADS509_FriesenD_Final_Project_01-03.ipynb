{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d1e3473e",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9225b1e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "import logging\n",
    "from newsapi import NewsApiClient\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import random\n",
    "import re\n",
    "import requests\n",
    "import string\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5ae5da6",
   "metadata": {},
   "source": [
    "## Source URLs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65b4ed93",
   "metadata": {},
   "outputs": [],
   "source": [
    "api_key = os.environ['NewsAPIKey']\n",
    "newsapi = NewsApiClient(api_key=api_key)\n",
    "\n",
    "sources = newsapi.get_sources(language='en', country='us')\n",
    "print(' - '.join([source['id'] for source in sources['sources']]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91d890a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def news_api_urls(q=None,\n",
    "                  s=None,\n",
    "                  d_from='2023-05-01',\n",
    "                  d_to='2023-05-31',\n",
    "                  api_lst=[]):\n",
    "    all_articles = newsapi.get_everything(q=q,\n",
    "                                          sources=s,\n",
    "                                          from_param=d_from,\n",
    "                                          to=d_to,\n",
    "                                          language='en',\n",
    "                                          sort_by='relevancy',\n",
    "                                          page=1)\n",
    "\n",
    "    for article in all_articles['articles']:\n",
    "        print('Title:', article['title'])\n",
    "\n",
    "    source_data01 = [(a['source']['name'],\n",
    "                      a['author'],\n",
    "                      a['title'],\n",
    "                      a['url'],\n",
    "                      a['publishedAt'],\n",
    "                      a['content'])\n",
    "                     for a in all_articles['articles']]\n",
    "\n",
    "    api_lst.extend(source_data01)\n",
    "    print(len(api_lst))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "320026d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Already executed:\n",
    "# run 'A': 'the-washington-post', '2023-05-30'\n",
    "# run 'B': 'the-washington-post', '2023-05-20/29'\n",
    "# run 'C': 'the-washington-post', '2023-05-21/22/23/24/25/26/27/28'\n",
    "# run 'D': 'the-washington-post', '2023-05-10/11/12/13/14/15/16/17/18/19'\n",
    "# run 'E': 'the-washington-post', '2023-05-05/06/07/08/09/31', '2023-06-01/02/03'\n",
    "# run 'F': 'fox-news', '2023-05-24/25/26/27/28'\n",
    "# run 'G': 'breitbart-news', '2023-05-24/25/26/27/28'\n",
    "# run 'H': 'cnn', '2023-05-24/25/26/27/28'\n",
    "\n",
    "# Last to execute:\n",
    "#run = 'H'\n",
    "#source_lst = ['cnn']\n",
    "#date_lst = ['2023-05-24', '2023-05-25', '2023-05-26', '2023-05-27', '2023-05-28']\n",
    "\n",
    "q_word_lst = ['justice OR surveillance', 'healthcare OR \"health care\"',\n",
    "              '(political AND (bias OR party)) OR republican OR democrat OR election',\n",
    "              'security AND (social OR national)']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89da81b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "api_record_lst01 = []\n",
    "for s in source_lst:\n",
    "    print(f'Source: {s}')\n",
    "    for d in date_lst:\n",
    "        print(f'Date: {d}')\n",
    "        for q in q_word_lst:\n",
    "            print(f'Query word: {q}')\n",
    "            time.sleep(5 * random.random())\n",
    "            news_api_urls(q=q,\n",
    "                          s=s,\n",
    "                          d_from=d,\n",
    "                          d_to=d,\n",
    "                          api_lst=api_record_lst01)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c3e861c",
   "metadata": {},
   "source": [
    "## Scrape articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb31d6a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "api_df = pd.DataFrame(api_record_lst01, columns=['source_name', 'author', 'title', 'url', 'publishedAt', 'content'])\n",
    "api_df['article_text'] = ''\n",
    "\n",
    "total_urls = len(api_df)\n",
    "for i, row in enumerate(api_df.itertuples(), 1):\n",
    "    print(f'Retrieving url {i} of {total_urls}...', end='')\n",
    "    response = requests.get(row.url)\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        print('; now Scraping...', end='')\n",
    "\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "        # The Washington Post\n",
    "#        try:\n",
    "#            script_tag = soup.find('script', {'type': 'application/ld+json'})\n",
    "#            article_json = json.loads(script_tag.string)\n",
    "#            article_content = article_json['hasPart']['value']\n",
    "#            api_df.at[row.Index, 'article_text'] = article_content\n",
    "      \n",
    "        # Fox News\n",
    "#        try:\n",
    "#            script_tag = soup.find('script', {'type': 'application/ld+json'})\n",
    "#            article_json = json.loads(script_tag.string)\n",
    "#            article_content = article_json['articleBody']\n",
    "#            api_df.at[row.Index, 'article_text'] = article_content\n",
    "\n",
    "        # Breitbart News\n",
    "#        try:\n",
    "#            title_tag = soup.find('h1')\n",
    "#            if title_tag is not None:\n",
    "#                title = title_tag.text\n",
    "#            content_tags = soup.find_all(['p', 'blockquote'])\n",
    "#            content = \" \".join([tag.text for tag in content_tags if tag.text.strip() != ''])\n",
    "#            api_df.at[row.Index, 'article_text'] = content\n",
    "\n",
    "        # CNN News\n",
    "        try:\n",
    "            script_tag = soup.find('script', {'type': 'application/ld+json'})\n",
    "            article_json = json.loads(script_tag.string)\n",
    "            article_content = article_json['articleBody']\n",
    "            api_df.at[row.Index, 'article_text'] = article_content\n",
    "\n",
    "        except KeyError:\n",
    "            print('; missing key in article JSON!', end='')\n",
    "\n",
    "        time.sleep(5 * random.random())\n",
    "        print('; done!')\n",
    "    else:\n",
    "        print(f'  response is {response.status_code}!')\n",
    "        \n",
    "api_df.to_csv(f'509_final_proj-{run}.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36a5799f",
   "metadata": {},
   "source": [
    "## Combine output files (*only at end of iterative process*)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21a431b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine all the output files\n",
    "master_df = pd.DataFrame()\n",
    "for letter in string.ascii_lowercase:\n",
    "    file_name = f'509_final_proj-{letter.upper()}.csv'\n",
    "    \n",
    "    if os.path.isfile(file_name):\n",
    "        df = pd.read_csv(file_name)\n",
    "\n",
    "        master_df = pd.concat([master_df, df], ignore_index=True)\n",
    "    else:\n",
    "        break\n",
    "print(master_df.info())\n",
    "\n",
    "# Get rid of what appear to be very cluttered (misread) article text rows\n",
    "pattern = re.compile('\\n{3,}')\n",
    "rows_list = []\n",
    "for index, row in master_df.iterrows():\n",
    "    try:\n",
    "        processed_row = row\n",
    "        if not pattern.search(str(processed_row[6])):\n",
    "            rows_list.append(processed_row)\n",
    "    except Exception as e:\n",
    "        print(f'Error processing row {index}: {e}')\n",
    "print(f'Rows processed = {len(rows_list)}')\n",
    "new_df = pd.concat(rows_list, axis=1).transpose()\n",
    "new_df.columns = master_df.columns\n",
    "              \n",
    "# Save the new file\n",
    "new_df.to_csv('509_final_proj.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acbe2bc7",
   "metadata": {},
   "source": [
    "## Combine all final files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f2eda90e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df0 = pd.read_csv('../data/509_final_proj.csv')\n",
    "df1 = pd.read_csv('../data/data_parsed_amc.csv')\n",
    "df2 = pd.read_csv('../data/News_API_FOX_CNN_Breitbert_May18_23_May31_3.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8bb5146a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1594 entries, 0 to 1593\n",
      "Data columns (total 7 columns):\n",
      " #   Column        Non-Null Count  Dtype \n",
      "---  ------        --------------  ----- \n",
      " 0   source_name   1594 non-null   object\n",
      " 1   author        1564 non-null   object\n",
      " 2   title         1593 non-null   object\n",
      " 3   url           1593 non-null   object\n",
      " 4   publishedAt   1593 non-null   object\n",
      " 5   content       1593 non-null   object\n",
      " 6   article_text  1561 non-null   object\n",
      "dtypes: object(7)\n",
      "memory usage: 87.3+ KB\n",
      "None\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 3390 entries, 0 to 3389\n",
      "Data columns (total 6 columns):\n",
      " #   Column          Non-Null Count  Dtype \n",
      "---  ------          --------------  ----- \n",
      " 0   source_name     3390 non-null   object\n",
      " 1   author          3375 non-null   object\n",
      " 2   title           3390 non-null   object\n",
      " 3   url             3390 non-null   object\n",
      " 4   publish_date    3390 non-null   object\n",
      " 5   article_parsed  3375 non-null   object\n",
      "dtypes: object(6)\n",
      "memory usage: 159.0+ KB\n",
      "None\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1302 entries, 0 to 1301\n",
      "Data columns (total 9 columns):\n",
      " #   Column          Non-Null Count  Dtype \n",
      "---  ------          --------------  ----- \n",
      " 0   Unnamed: 0.1    1302 non-null   int64 \n",
      " 1   Unnamed: 0      1302 non-null   int64 \n",
      " 2   Source          1302 non-null   object\n",
      " 3   Author          1302 non-null   object\n",
      " 4   Title           1302 non-null   object\n",
      " 5   URL             1302 non-null   object\n",
      " 6   date            1302 non-null   object\n",
      " 7   content         1302 non-null   object\n",
      " 8   article_parsed  1295 non-null   object\n",
      "dtypes: int64(2), object(7)\n",
      "memory usage: 91.7+ KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(df0.info())\n",
    "print(df1.info())\n",
    "print(df2.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d98d1e6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 4509 entries, 0 to 4983\n",
      "Data columns (total 7 columns):\n",
      " #   Column        Non-Null Count  Dtype \n",
      "---  ------        --------------  ----- \n",
      " 0   source_name   4509 non-null   object\n",
      " 1   author        4472 non-null   object\n",
      " 2   title         4509 non-null   object\n",
      " 3   url           4509 non-null   object\n",
      " 4   publish_date  4509 non-null   object\n",
      " 5   content       1158 non-null   object\n",
      " 6   article_text  4508 non-null   object\n",
      "dtypes: object(7)\n",
      "memory usage: 281.8+ KB\n"
     ]
    }
   ],
   "source": [
    "df0 = df0.rename(columns={'publishedAt': 'publish_date'})\n",
    "\n",
    "df1 = df1.rename(columns={'article_parsed': 'article_text'})\n",
    "df1['content'] = np.nan\n",
    "\n",
    "df2 = df2.drop(columns=['Unnamed: 0.1', 'Unnamed: 0'])\n",
    "df2 = df1.rename(columns={'Source': 'source_name',\n",
    "                          'Author': 'author',\n",
    "                          'Title': 'title',\n",
    "                          'URL': 'url',\n",
    "                          'date': 'publish_date',\n",
    "                          'article_parsed': 'article_text'})\n",
    "\n",
    "df = pd.concat([df0, df1, df2], ignore_index=True)\n",
    "\n",
    "df = df.drop_duplicates(subset='article_text')\n",
    "df.info()\n",
    "\n",
    "df.to_csv('../data/master.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a85a3637",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
